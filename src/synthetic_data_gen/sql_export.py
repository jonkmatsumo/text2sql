"""SQL export module for compose-compatible bootstrap.

Generates 01-schema.sql and 02-data.sql files for docker-entrypoint-initdb.d.
"""

from __future__ import annotations

import json
import logging
from pathlib import Path
from typing import TYPE_CHECKING, Any, Dict

import pandas as pd

if TYPE_CHECKING:
    from synthetic_data_gen.context import GenerationContext

from synthetic_data_gen.schema import GENERATION_ORDER

logger = logging.getLogger(__name__)


# Mapping from pandas dtype to PostgreSQL type
DTYPE_TO_PG = {
    "int64": "BIGINT",
    "int32": "INTEGER",
    "float64": "DOUBLE PRECISION",
    "float32": "REAL",
    "bool": "BOOLEAN",
    "datetime64[ns]": "TIMESTAMP",
    "object": "TEXT",
    "string": "TEXT",
}


def _infer_pg_type(column_name: str, dtype: str, sample_values: pd.Series) -> str:
    """Infer PostgreSQL type from pandas dtype and column name heuristics."""
    dtype_str = str(dtype)

    # Handle common naming patterns
    if column_name.endswith("_id") or column_name == "id":
        # Check if it's actually an integer ID
        if "int" in dtype_str:
            return "BIGINT"
        return "TEXT"  # UUID-style IDs

    if column_name.endswith("_ts") or column_name.endswith("_date"):
        return "TIMESTAMP"

    if column_name.endswith("_amount") or column_name.endswith("_fee"):
        return "DECIMAL(15, 2)"

    if column_name.startswith("is_") or dtype_str == "bool":
        return "BOOLEAN"

    # Handle datetime
    if "datetime" in dtype_str:
        return "TIMESTAMP"

    # Use mapping
    return DTYPE_TO_PG.get(dtype_str, "TEXT")


def _escape_sql_value(value: Any) -> str:
    """Escape a Python value for SQL INSERT."""
    if pd.isna(value):
        return "NULL"
    if isinstance(value, bool):
        return "TRUE" if value else "FALSE"
    if isinstance(value, (int, float)):
        return str(value)
    if isinstance(value, (dict, list)):
        # JSON data
        json_str = json.dumps(value).replace("'", "''")
        return f"'{json_str}'"
    # String - escape single quotes
    str_val = str(value).replace("'", "''")
    return f"'{str_val}'"


def generate_schema_sql(ctx: "GenerationContext", target_schema: str = "public") -> str:
    """Generate CREATE TABLE statements for all tables in generation order.

    Args:
        ctx: Generation context with tables.
        target_schema: Target schema name (default: public).

    Returns:
        SQL string with CREATE TABLE statements.
    """
    lines = [
        "-- Generated by text2sql-synth",
        "-- Schema DDL for synthetic data",
        "",
        f"SET search_path TO {target_schema};",
        "",
    ]

    for table_name in GENERATION_ORDER:
        if table_name not in ctx.tables:
            continue

        df = ctx.tables[table_name]
        columns = []

        for col_name in df.columns:
            pg_type = _infer_pg_type(col_name, df[col_name].dtype, df[col_name])
            columns.append(f"    {col_name} {pg_type}")

        lines.append(f"CREATE TABLE IF NOT EXISTS {table_name} (")
        lines.append(",\n".join(columns))
        lines.append(");")
        lines.append("")

    return "\n".join(lines)


def generate_data_sql(
    ctx: "GenerationContext",
    target_schema: str = "public",
    batch_size: int = 100,
) -> str:
    """Generate INSERT statements for all tables in generation order.

    Args:
        ctx: Generation context with tables.
        target_schema: Target schema name (default: public).
        batch_size: Number of rows per INSERT statement.

    Returns:
        SQL string with INSERT statements.
    """
    lines = [
        "-- Generated by text2sql-synth",
        "-- Data DML for synthetic data",
        "",
        f"SET search_path TO {target_schema};",
        "",
        "BEGIN;",
        "",
    ]

    for table_name in GENERATION_ORDER:
        if table_name not in ctx.tables:
            continue

        df = ctx.tables[table_name]
        if df.empty:
            continue

        lines.append(f"-- {table_name}: {len(df)} rows")
        columns_list = ", ".join(df.columns)

        # Generate batched INSERTs
        for i in range(0, len(df), batch_size):
            batch_df = df.iloc[i : i + batch_size]
            values_list = []

            for _, row in batch_df.iterrows():
                values = ", ".join(_escape_sql_value(v) for v in row)
                values_list.append(f"({values})")

            lines.append(f"INSERT INTO {table_name} ({columns_list}) VALUES")
            lines.append(",\n".join(values_list) + ";")
            lines.append("")

    lines.append("COMMIT;")
    lines.append("")

    return "\n".join(lines)


def export_sql_files(
    ctx: "GenerationContext",
    out_dir: str | Path,
    target_schema: str = "public",
) -> Dict[str, Path]:
    """Export SQL files for compose bootstrap.

    Generates:
    - 01-schema.sql: CREATE TABLE statements
    - 02-data.sql: INSERT statements

    Args:
        ctx: Generation context with tables.
        out_dir: Output directory.
        target_schema: Target schema name (default: public).

    Returns:
        Dict mapping file type to path.
    """
    out_path = Path(out_dir)
    out_path.mkdir(parents=True, exist_ok=True)

    schema_path = out_path / "01-schema.sql"
    data_path = out_path / "02-data.sql"

    logger.info("Generating schema SQL...")
    schema_sql = generate_schema_sql(ctx, target_schema)
    with open(schema_path, "w") as f:
        f.write(schema_sql)
    logger.info("Wrote %s", schema_path)

    logger.info("Generating data SQL...")
    data_sql = generate_data_sql(ctx, target_schema)
    with open(data_path, "w") as f:
        f.write(data_sql)
    logger.info("Wrote %s", data_path)

    return {"schema": schema_path, "data": data_path}
