# Text 2 SQL - Cursor Rules

## Project Context
This is a Text 2 SQL MVP (Phase 1) that implements a secure, containerized data access layer using the Model Context Protocol (MCP). The system connects an LLM agent to a PostgreSQL database (Pagila dataset) through a Python MCP server.

## Architecture Principles
- **Decoupling**: The AI agent (Brain) is decoupled from the data warehouse (Tool) via MCP
- **Security First**: Multi-layered security (container isolation, least privilege DB roles, application-level gates)
- **Read-Only**: All database access is strictly read-only
- **Docker-First**: All services run in containers orchestrated via Docker Compose

## Technology Stack
- **Database**: PostgreSQL 16 (Alpine)
- **MCP Server**: Python 3.12 with fastmcp framework
- **Database Driver**: asyncpg (async PostgreSQL driver)
- **Transport**: Server-Sent Events (SSE) for Docker compatibility
- **Validation**: Pydantic for tool input schemas

## Code Style
- Use async/await patterns for all database operations
- Implement comprehensive error handling (return errors as strings for LLM self-correction)
- Use type hints throughout
- Follow PEP 8 for Python code
- Use descriptive variable names (e.g., `bi_agent_ro` not `user1`)

## Security Requirements
- All SQL queries must pass regex pre-flight checks for mutative keywords
- Database user must have NO write privileges (SELECT only)
- Error messages must be sanitized (no internal paths, credentials)
- Result sets limited to 1000 rows to prevent context window overflow

## File Structure
```
text2sql/
├── docker-compose.yml
├── .env
├── .pre-commit-config.yaml
├── .github/
│   ├── workflows/
│   │   ├── ci.yml
│   │   └── test.yml
│   └── dependabot.yml
├── database/
│   ├── Dockerfile (if needed)
│   └── init-scripts/
│       ├── 01-schema.sql
│       ├── 02-data.sql
│       └── 03-permissions.sql
└── mcp-server/
    ├── Dockerfile
    ├── pyproject.toml
    └── src/
        ├── main.py
        ├── db.py
        └── tools.py
```

## Implementation Priorities
1. **Sprint 1**: Infrastructure & Database (PostgreSQL with Pagila dataset, pre-commit hooks, GitHub Actions)
2. **Sprint 2**: MCP Server Development (4 tools: list_tables, get_table_schema, execute_sql_query, get_semantic_definitions)
3. **Sprint 3**: Integration & Verification (Docker Compose, end-to-end testing)

## Code Quality & CI/CD
- **Pre-commit hooks**: Enforce code formatting (black, isort), linting (flake8), and file checks
- **GitHub Actions**: Automated CI/CD for linting, Docker builds, database initialization tests, and security scanning
- **Code Style**: Black formatting (100 char line length), isort for imports, flake8 for linting
- All code must pass pre-commit hooks before committing

### Running Pre-commit Hooks
- **Important**: Pre-commit may not be in PATH. Use: `python3 -m pre_commit run --all-files`
- If pre-commit is not installed, install it first: `python3 -m pip install pre-commit`
- Pre-commit hooks will auto-format files (black, isort). Re-run to verify all hooks pass after auto-fixes
- Always run pre-commit hooks after making changes to source code, before running tests

## Testing Approach
- Verify database health via docker ps and direct SQL connection
- Test MCP server using @modelcontextprotocol/inspector or curl
- Security verification: Attempt injection attacks, verify read-only enforcement

### Development Workflow
- **ALWAYS run pre-commit hooks AND tests after making changes to source code**
- Workflow: Make changes → Run pre-commit → Run tests → Verify all pass
- Pre-commit command: `python3 -m pre_commit run --all-files`
- Test command: `./venv/bin/pytest mcp-server/tests/ -v` (or specific test file/class)
- If pre-commit auto-fixes files, re-run it to verify all hooks pass
- Never commit code that fails pre-commit hooks or tests

## Common Patterns
- Database connections: Use asyncpg connection pool, always close connections in finally blocks
- Tool responses: Return JSON-serializable data (use `json.dumps` with `default=str` for dates/decimals)
- Error handling: Catch `asyncpg.PostgresError` and return error message as string (not exception)
- Schema queries: Always filter by `table_schema = 'public'` to exclude system catalogs
